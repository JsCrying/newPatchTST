Args in experiment:
rnn_base_model = 'FreTS'
Namespace(random_seed=2021, is_training=1, model_id='96_720', model='FreTS_baseline', baseline=True, data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=18, pred_len=720, fc_dropout=0.2, head_dropout=0.0, patch_len=45, stride=30, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=30, patience=10, learning_rate=0.000249, des='Exp', loss='mse', lradj='TST', pct_start=0.2, use_amp=False, momentum=0.9669740372386173, frequency=0.05, sampling_rate=100, use_norm=True, hidden_size=2946, exp_name='MTSF', channel_independence=False, inverse=False, class_strategy='projection', target_root_path='./data/electricity/', target_data_path='electricity.csv', efficient_training=False, partial_start_index=0, top_k=5, num_kernels=6, use_gpu=True, gpu=0, use_multi_gpu=True, devices='0,1', test_flop=False, x=5, rnn_base_model='FreTS', debug=False, dvices='0,1', device_ids=[0, 1])
Use GPU: cuda:0
>>>>>>>start training : 96_720_FreTS_baseline_custom_ftM_sl96_ll18_pl720_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1037
test 2789
	iters: 100, epoch: 1 | loss: 0.8580085
	speed: 0.4086s/iter; left time: 15569.7861s
	iters: 200, epoch: 1 | loss: 0.7400793
	speed: 0.3946s/iter; left time: 14995.6670s
	iters: 300, epoch: 1 | loss: 0.6610519
	speed: 0.3951s/iter; left time: 14975.4109s
Epoch: 1 cost time: 152.375741481781
[2024-04-23 10:14:10] [32mIntermediate result: 0.99840504  (Index 0)[0m
Epoch: 1, Steps: 382 | Train Loss: 0.7657968 Vali Loss: 0.8113399 Test Loss: 0.9984050
Validation loss decreased (inf --> 0.811340).  Saving model ...
Updating learning rate to 1.1431874021657355e-05
	iters: 100, epoch: 2 | loss: 0.5437112
	speed: 1.1518s/iter; left time: 43446.3901s
	iters: 200, epoch: 2 | loss: 0.5147765
	speed: 0.3951s/iter; left time: 14865.0805s
	iters: 300, epoch: 2 | loss: 0.4832252
	speed: 0.3953s/iter; left time: 14832.2428s
Epoch: 2 cost time: 151.23271989822388
[2024-04-23 10:17:24] [32mIntermediate result: 0.85526896  (Index 1)[0m
Epoch: 2, Steps: 382 | Train Loss: 0.5246996 Vali Loss: 0.6915537 Test Loss: 0.8552690
Validation loss decreased (0.811340 --> 0.691554).  Saving model ...
Updating learning rate to 1.581124419346299e-05
	iters: 100, epoch: 3 | loss: 0.4472853
	speed: 1.1550s/iter; left time: 43125.2751s
	iters: 200, epoch: 3 | loss: 0.4547392
	speed: 0.3957s/iter; left time: 14733.7084s
	iters: 300, epoch: 3 | loss: 0.4404977
	speed: 0.3952s/iter; left time: 14678.3463s
Epoch: 3 cost time: 151.46630764007568
[2024-04-23 10:20:39] [32mIntermediate result: 0.7907486  (Index 2)[0m
Epoch: 3, Steps: 382 | Train Loss: 0.4624172 Vali Loss: 0.6446981 Test Loss: 0.7907486
Validation loss decreased (0.691554 --> 0.644698).  Saving model ...
Updating learning rate to 2.2990247711081107e-05
	iters: 100, epoch: 4 | loss: 0.4456114
	speed: 1.1553s/iter; left time: 42692.4045s
